ğŸ“¦ Smart Product Pricing â€” Amazon ML Challenge 2025

Multimodal Deep Learning Pipeline (CLIP + SigLIP + Blended MLP)

This repository contains the full multimodal solution developed for the Amazon ML Challenge 2025 â€“ Smart Product Pricing Task.
The goal is to predict the optimal price of 75,000+ e-commerce products using:

Product images

Product catalog descriptions

Structured fields (value, unit, brand, pack size)

Robust outlier handling

CLIP + SigLIP embeddings

A blended MLP regressor with EMA stabilization

This approach secured a Top-15 Public Leaderboard Rank (#14, SMAPE: 40.858%).

ğŸš€ Project Structure
â”œâ”€â”€ preprocess.py                    # Text cleanup + unit extraction + parquet generator
â”œâ”€â”€ extract_clip_features.py         # CLIP (ViT-B/32) embeddings
â”œâ”€â”€ extract_siglip_features.py       # SigLIP embeddings
â”œâ”€â”€ train_clip_siglip_blend_v2.py    # Final blended model training (MLP + outlier handling)
â”œâ”€â”€ utils.py                         # Seed, CSV safety, image downloader, helpers
â”œâ”€â”€ data/                            # Raw + processed data + embeddings
â””â”€â”€ models/                          # Saved models + predictions

1ï¸âƒ£ Data Preprocessing (preprocess.py)

preprocess

âœ” Cleans & normalizes product text

Unicode normalization

Fixes encoding artifacts

Removes HTML tags

Compresses whitespace

âœ” Extracts structured fields

Value + unit extraction (oz, lb, ml, g, etc.)

Pack size extraction (Pack of 6, 12 per case, â€¦)

Brand extraction

Item name extraction

âœ” Text statistics

length, word count, sentence count

has_image flag

âœ” Price transformation

log(price) stored as price_log

âœ” Output

Creates:

data/processed_train.parquet
data/processed_test.parquet

â–¶ Run:
python preprocess.py --train_path data/train.csv --test_path data/test.csv --out_dir data/

2ï¸âƒ£ CLIP Feature Extraction (extract_clip_features.py)

extract_clip_features

Extracts 512-dim text + 512-dim image embeddings using:
openai/clip-vit-base-patch32

Features generated:

clip_text_embeddings_{train|test}.npy

clip_image_embeddings_{train|test}.npy

clip_similarity_{train|test}.npy (cosine similarity)

ID â†’ index mapping JSON

â–¶ Run:
python extract_clip_features.py \
    --input data/processed_train.parquet \
    --image_dir data/images \
    --out_dir data/

3ï¸âƒ£ SigLIP Feature Extraction (extract_siglip_features.py)

extract_siglip_features

Uses google/siglip-base-patch16-224 for improved cross-modal embeddings.

Outputs:

siglip_text_embeddings_{train|test}.npy

siglip_image_embeddings_*.npy

siglip_similarity_*.npy

ID mapping JSON

Includes:

Safe image loading

Zero-vector fallback

Text truncation

â–¶ Run:
python extract_siglip_features.py \
    --train_data data/processed_train.parquet \
    --test_data data/processed_test.parquet \
    --image_dir data/images \
    --output_dir data/

4ï¸âƒ£ Multimodal Feature Engineering (train_clip_siglip_blend_v2.py)

train_clip_siglip_blend_v2

This script blends four embedding vectors:

CLIP image

CLIP text

SigLIP image

SigLIP text

Plus:

âœ” Cross-modal similarity

Cosine similarity

Mean/Std/Max/Min difference

Elementwise products

âœ” Statistical comparisons

Norms, means, std-devs of each embedding
Image vs text statistical deltas
Image vs text statistical ratios

âœ” SigLIP similarity scores included

Total dimensionality â‰ˆ ~2600+

5ï¸âƒ£ Advanced Outlier Handling

Integrated inside training script.

âœ” Multi-level outlier processing:

Price clipping (1stâ€“99th percentile)

Winsorization on 2600-dim features

Isolation Forest (optional anomaly detection)

Quantile Transformation for heavy-tailed distributions

RobustScaler normalization

This dramatically stabilizes training and improves SMAPE.

6ï¸âƒ£ Model Architecture
ğŸ§  Residual MLP Regressor

Backbone: 1024 â†’ 512 â†’ 256

Two residual blocks

GELU activations

Dropout=0.20

Weight Decay + AdamW

Gradient clipping

EMA (Exponential Moving Average) for stable predictions

Training:

5-fold K-Fold

Huber Loss + L1

ReduceLROnPlateau

Automatic Mixed Precision (AMP)

7ï¸âƒ£ Training the Model
â–¶ Run full Blend-V2 Training:
python train_clip_siglip_blend_v2.py \
    --train_data data/processed_train.parquet \
    --test_data data/processed_test.parquet \
    --out_dir models/clip_siglip_blend_v2 \
    --device cuda:0

Output files:

fold_*_best.pt (best checkpoints)

preprocessing.pkl

config.json

oof.csv

test_predictions.csv

ğŸ“Š Performance
Metric	Score
SMAPE (public leaderboard)	40.858%
Rank	#14

Reasons for strong performance:

CLIP + SigLIP cross-modal synergy

Rich feature engineering

Aggressive outlier handling

EMA-stabilized MLP

ğŸ–¼ Example: Complete Pipeline
# 1. Preprocess
python preprocess.py

# 2. Download images
python utils.py --download_images

# 3. Extract CLIP + SigLIP
python extract_clip_features.py
python extract_siglip_features.py

# 4. Train final blended model
python train_clip_siglip_blend_v2.py

ğŸ“ Directory Expectations
data/
  train.csv
  test.csv
  images/
  processed_train.parquet
  processed_test.parquet
  clip_text_embeddings_train.npy
  ...
models/
  clip_siglip_blend_v2/
      fold_1_best.pt
      test_predictions.csv

ğŸ™Œ Credits

This work was developed as part of the Amazon ML Challenge 2025.
The pipeline is fully reproducible and uses only local, license-compliant models.
